{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories of this experiment are /home/shray/HT-SELEX/HT-SELEX-40mer-counts/ZFP3_GC40NAGAGTT_Lysate_BatchAATA_Cycle1_R1.40mers.counts /home/shray/HT-SELEX/HT-SELEX-40mer-counts/ZFP3_GC40NAGAGTT_Lysate_BatchAATA_Cycle2_R1.40mers.counts /home/shray/HT-SELEX/HT-SELEX-40mer-counts/ZFP3_GC40NAGAGTT_Lysate_BatchAATA_Cycle3_R1.40mers.counts\n",
      "Sizes 1, 2, and 3 are 23822030 23312171 16338762\n",
      "23822030\n",
      "Dimensions of kmer_Sequences_Train_ForSure: 6003 40 4\n",
      "Dimensions of kmer_Sequences 54000\n",
      "Dimensions of count_Vals_Train_ForSure: 6003 3\n",
      "['3.0', -2, -2]\n",
      "--- 3.388583183288574 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# @author Shray Alag \n",
    "# @version 07.29.2020\n",
    "# @mentors Abhi, Anshul\n",
    "# @lab Kundaje Lab\n",
    "#\n",
    "# This notebook will taken in an HT-SELEX experiment and will split up the data inputs, \n",
    "# create a CNN, train the CNN, and evaluate it. This first cell is for establishing the path \n",
    "# directories, taking in the data, and filling up the kmer_Sequences (i.e. \"ATTGCGG\"), \n",
    "# count_Vals (i.e. \"1403\"), and probability_Vals (i.e. \"0.0021591\").\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import statements\n",
    "from keras.callbacks import History, Callback, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#path directories\n",
    "#To use this notebook simply change the experiment name and path directories to your liking. \n",
    "#Mostly just change the following variables and make sure your path directories are set up properly\n",
    "#Also make sure to unzip everything\n",
    "experiment_BASE_Name = \"ZFP3_GC40NAGAGTT_Lysate_BatchAATA_Cycle\"\n",
    "num_Of_Kmers = 40\n",
    "\n",
    "\n",
    "experiment_1_Name = experiment_BASE_Name + \"1_R1.\" + str(num_Of_Kmers) + \"mers.counts\"\n",
    "experiment_2_Name = experiment_BASE_Name + \"2_R1.\" + str(num_Of_Kmers) + \"mers.counts\"\n",
    "experiment_3_Name = experiment_BASE_Name + \"3_R1.\" + str(num_Of_Kmers) + \"mers.counts\"\n",
    "\n",
    "\n",
    "\n",
    "BASE_PATH_DIR = \"/home/shray/HT-SELEX\"\n",
    "DATA_FOLDER_NAME = \"HT-SELEX-\" + str(num_Of_Kmers) + \"mer-counts\"\n",
    "DATA_PATH_DIR = BASE_PATH_DIR + \"/\" + DATA_FOLDER_NAME\n",
    "\n",
    "\n",
    "EXPERIMENT_DIR_1 = DATA_PATH_DIR + \"/\" + experiment_1_Name\n",
    "EXPERIMENT_DIR_2 = DATA_PATH_DIR + \"/\" + experiment_2_Name\n",
    "EXPERIMENT_DIR_3 = DATA_PATH_DIR + \"/\" + experiment_3_Name\n",
    "\n",
    "\n",
    "print(\"Directories of this experiment are\", EXPERIMENT_DIR_1, EXPERIMENT_DIR_2, EXPERIMENT_DIR_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Global Vars\n",
    "NUM_OF_SAMPLES_MUST_BE_IN_TRAINING = 2000\n",
    "start_time = time.time()\n",
    "global_time = time.time()\n",
    "extra = np.zeros([num_Of_Kmers, 1])\n",
    "counterFor3BasePairs = 0;\n",
    "mask_value = -2;\n",
    "keepInTraining = True\n",
    "breakVal = 20000\n",
    "\n",
    "\n",
    "#Overarching methods\n",
    "def printOutArrays():\n",
    "    # Prints out the shapes of the arrays\n",
    "    print (\"X_train shape: \" + str(x_train.shape))\n",
    "    print (\"y_train shape: \" + str(y_train.shape))\n",
    "    print (\"X_test shape: \" + str(x_test.shape))\n",
    "    print (\"y_test shape: \" + str(y_test.shape))\n",
    "    print (\"X_val shape: \" + str(x_hidden.shape))\n",
    "    print (\"y_val shape: \" + str(y_hidden.shape))\n",
    "    \n",
    "def dimensionsOfInputs():\n",
    "    dim1 = len(kmer_Sequences_Train_ForSure)\n",
    "    dim2 = len(kmer_Sequences_Train_ForSure[0])\n",
    "    dim3 = len(kmer_Sequences_Train_ForSure[0][0])\n",
    "    print(\"Dimensions of kmer_Sequences_Train_ForSure:\", dim1, dim2, dim3)\n",
    "    dim1 = len(kmer_Sequences)\n",
    "    print(\"Dimensions of kmer_Sequences\", dim1)\n",
    "    dim1 = len(count_Vals_Train_ForSure)\n",
    "    dim2 = len(count_Vals_Train_ForSure[0])\n",
    "    print(\"Dimensions of count_Vals_Train_ForSure:\", dim1, dim2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# setting up the numpy arrays\n",
    "# Following commented code is just for the user to see the data structures being used\n",
    "kmer_Sequences_Train_ForSure = []\n",
    "# kmer_Sequences_Train_ForSure = np.array(kmer_Sequences_Train_ForSure)\n",
    "count_Vals_Train_ForSure = []\n",
    "probability_Vals_Train_ForSure = []\n",
    "\n",
    "kmer_Sequences = []\n",
    "count_Vals = []\n",
    "probability_Vals = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#reading through and getting the training data + doing one hot encoding for the ones that are for sure in training\n",
    "experimentReader1 = open(EXPERIMENT_DIR_1, \"r\")\n",
    "experimentReader2 = open(EXPERIMENT_DIR_2, \"r\")\n",
    "experimentReader3 = open(EXPERIMENT_DIR_3, \"r\")\n",
    "\n",
    "experimentReader1.seek(0, os.SEEK_END)\n",
    "size1 = experimentReader1.tell()\n",
    "\n",
    "experimentReader2.seek(0, os.SEEK_END)\n",
    "size2 = experimentReader2.tell()\n",
    "\n",
    "experimentReader3.seek(0, os.SEEK_END)\n",
    "size3 = experimentReader3.tell()\n",
    "\n",
    "print(\"Sizes 1, 2, and 3 are\", size1, size2, size3)\n",
    "maxLen = max(size1, size2, size3)\n",
    "print(maxLen)\n",
    "\n",
    "\n",
    "experimentReader1 = open(EXPERIMENT_DIR_1, \"r\")\n",
    "experimentReader2 = open(EXPERIMENT_DIR_2, \"r\")\n",
    "experimentReader3 = open(EXPERIMENT_DIR_3, \"r\")\n",
    "\n",
    "experimentReader1.readline() #don't want the first line with the headers (i.e. \"k_mers\", \"counts\") going into the data\n",
    "experimentReader2.readline()\n",
    "experimentReader3.readline()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(maxLen):\n",
    "\n",
    "    \n",
    "    if i < size1:\n",
    "        line1 = experimentReader1.readline()\n",
    "        elements = line1.split()\n",
    "    \n",
    "    \n",
    "        if keepInTraining == True:\n",
    "            count_Vals_Train_ForSure.append([elements[1], mask_value, mask_value])\n",
    "            probability_Vals_Train_ForSure.append([elements[2], mask_value, mask_value])\n",
    "\n",
    "            #One-hot encoding for the kmer Sequence\n",
    "            stringTemp = list(elements[0].rstrip())  \n",
    "            s = pd.Series(stringTemp)\n",
    "            temp = pd.get_dummies(s)\n",
    "\n",
    "            if temp.shape[1] == 3:\n",
    "                counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "                temp = np.append(temp, extra, 1)\n",
    "\n",
    "            if temp.shape[1] == 4 and temp.shape[0] == num_Of_Kmers:\n",
    "                kmer_Sequences_Train_ForSure.append(np.array(temp))\n",
    "    \n",
    "\n",
    "        elif len(elements) >= 3:\n",
    "            count_Vals.append([elements[1], mask_value, mask_value])\n",
    "            probability_Vals.append([elements[2], mask_value, mask_value])\n",
    "            kmer_Sequences.append(elements[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if i < size2:\n",
    "        line2 = experimentReader2.readline()\n",
    "        elements = line2.split()\n",
    "    \n",
    "        if keepInTraining == True and len(elements) >= 3:\n",
    "            count_Vals_Train_ForSure.append([mask_value, elements[1], mask_value])\n",
    "            probability_Vals_Train_ForSure.append([mask_value, elements[2], mask_value])\n",
    "\n",
    "            #One-hot encoding for the kmer Sequence\n",
    "            stringTemp = list(elements[0].rstrip())  \n",
    "            s = pd.Series(stringTemp)\n",
    "            temp = pd.get_dummies(s)\n",
    "\n",
    "            if temp.shape[1] == 3:\n",
    "                counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "                temp = np.append(temp, extra, 1)\n",
    "\n",
    "            if temp.shape[1] == 4 and temp.shape[0] == num_Of_Kmers:\n",
    "                kmer_Sequences_Train_ForSure.append(np.array(temp))\n",
    "\n",
    "        elif len(elements) >= 3:\n",
    "            count_Vals.append([mask_value, elements[1], mask_value])\n",
    "            probability_Vals.append([mask_value, elements[2], mask_value])\n",
    "\n",
    "            kmer_Sequences.append(elements[0])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    if i < size3:\n",
    "        line3 = experimentReader3.readline()\n",
    "        elements = line3.split()\n",
    "    \n",
    "        if keepInTraining == True and len(elements) >= 3:\n",
    "            count_Vals_Train_ForSure.append([mask_value, mask_value, elements[1]])\n",
    "            probability_Vals_Train_ForSure.append([mask_value, mask_value, elements[2]])\n",
    "\n",
    "            #One-hot encoding for the kmer Sequence\n",
    "            stringTemp = list(elements[0].rstrip())  \n",
    "            s = pd.Series(stringTemp)\n",
    "            temp = pd.get_dummies(s)\n",
    "\n",
    "            if temp.shape[1] == 3:\n",
    "                counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "                temp = np.append(temp, extra, 1)\n",
    "\n",
    "            if temp.shape[1] == 4 and temp.shape[0] == num_Of_Kmers:\n",
    "                kmer_Sequences_Train_ForSure.append(np.array(temp))\n",
    "\n",
    "        elif len(elements) >= 3:\n",
    "            count_Vals.append([mask_value, mask_value, elements[1]])\n",
    "            probability_Vals.append([mask_value, mask_value, elements[2]])\n",
    "            kmer_Sequences.append(elements[0])\n",
    "\n",
    "            \n",
    "    if i >= NUM_OF_SAMPLES_MUST_BE_IN_TRAINING:\n",
    "        keepInTraining = False\n",
    "        \n",
    "    if i >= breakVal:\n",
    "        break\n",
    "        \n",
    "\n",
    "dimensionsOfInputs()\n",
    "print(count_Vals_Train_ForSure[0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.0', -2, -2]\n",
      "3.0\n",
      "1.1.0\n",
      "51 number of reads only had 3 base pairs\n",
      "Kmer Sequences shape: (6003, 40, 4)\n",
      "Counts shape: (6003, 3)\n",
      "X_train shape: (37800, 40, 4)\n",
      "y_train shape: (37800, 3)\n",
      "X_test shape: (8100, 40, 4)\n",
      "y_test shape: (8100, 3)\n",
      "X_val shape: (8100, 40, 4)\n",
      "y_val shape: (8100, 3)\n",
      "Kmer Sequences shape: (6003, 40, 4)\n",
      "Counts shape: (6003, 3)\n",
      "-----------\n",
      "X_train shape: (43803, 40, 4)\n",
      "y_train shape: (43803, 3)\n",
      "X_test shape: (8100, 40, 4)\n",
      "y_test shape: (8100, 3)\n",
      "X_val shape: (8100, 40, 4)\n",
      "y_val shape: (8100, 3)\n",
      "--- 29.57035255432129 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#expect this cell to take a few minutes\n",
    "\n",
    "print(count_Vals_Train_ForSure[0])\n",
    "print(count_Vals_Train_ForSure[0][0])\n",
    "\n",
    "#import statements\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "print(pd.__version__)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the datasets into three parts - train, test, and hidden with a 70%, 15%, 15% split\n",
    "# test datasets will be used to select the best model. \n",
    "# Hidden dataset will be used for final accuracy\n",
    "# Makes sure top 2000 counts are in training set as it is important that the model learns these prominent sequences\n",
    "x_train_temp, x_other, y_train, y_other = train_test_split(kmer_Sequences, count_Vals, test_size=0.3, random_state=42)\n",
    "x_test_temp, x_hidden_temp, y_test, y_hidden = train_test_split(x_other, y_other, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Doing one hot encoding for the rest of the data\n",
    "x_train = np.zeros([len(x_train_temp), num_Of_Kmers, 4])\n",
    "x_test = np.zeros([len(x_test_temp), num_Of_Kmers, 4])\n",
    "x_hidden = np.zeros([len(x_hidden_temp), num_Of_Kmers, 4])\n",
    "\n",
    "\n",
    "for i in range(len(x_train_temp)):\n",
    "    stringTemp = list(x_train_temp[i])\n",
    "    s = pd.Series(stringTemp)\n",
    "    temp = pd.get_dummies(s)\n",
    "    \n",
    "    if temp.shape[1] == 3:\n",
    "        counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "        temp = np.append(temp, extra, 1)\n",
    "            \n",
    "    if temp.shape[1] == 4:\n",
    "        x_train[i] = temp\n",
    "    \n",
    "    if i < len(x_test_temp):\n",
    "        stringTemp = list(x_test_temp[i])\n",
    "        s = pd.Series(stringTemp)\n",
    "        temp1 = pd.get_dummies(s)\n",
    "        \n",
    "        \n",
    "        if temp1.shape[1] == 3:\n",
    "            counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "            temp1 = np.append(temp1, extra, 1)\n",
    "            \n",
    "        if temp1.shape[1] == 4:\n",
    "            x_test[i] = temp1 \n",
    "    \n",
    "    if i < len(x_hidden_temp):\n",
    "        stringTemp = list(x_hidden_temp[i])\n",
    "        s = pd.Series(stringTemp)\n",
    "        temp2 = pd.get_dummies(s)\n",
    "        \n",
    "        if temp2.shape[1] == 3 and temp2.shape[0] == num_Of_Kmers:\n",
    "            counterFor3BasePairs = counterFor3BasePairs + 1\n",
    "            temp2 = np.append(temp2, extra, 1)\n",
    "        \n",
    "        if temp2.shape[1] == 4:\n",
    "            x_hidden[i] = temp2\n",
    "        \n",
    "print(\"%s number of reads only had 3 base pairs\" %counterFor3BasePairs)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#convert arrays to numpy\n",
    "kmer_Sequences_Train_ForSure = np.array(kmer_Sequences_Train_ForSure)\n",
    "count_Vals_Train_ForSure = np.array(count_Vals_Train_ForSure)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_hidden = np.array(y_hidden)\n",
    "\n",
    "#making sure the one hot encoding worked \n",
    "print (\"Kmer Sequences shape: \" + str(kmer_Sequences_Train_ForSure.shape))\n",
    "print (\"Counts shape: \" + str(count_Vals_Train_ForSure.shape))\n",
    "printOutArrays()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#combines the training for sure samples and the training samples\n",
    "x_train = np.concatenate((x_train, kmer_Sequences_Train_ForSure))\n",
    "y_train = np.concatenate((y_train, count_Vals_Train_ForSure))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print statements to make sure it worked\n",
    "print (\"Kmer Sequences shape: \" + str(kmer_Sequences_Train_ForSure.shape))\n",
    "print (\"Counts shape: \" + str(count_Vals_Train_ForSure.shape))\n",
    "print(\"-----------\")\n",
    "printOutArrays()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 5]\n",
      "[2 1 1 0 3 2]\n",
      "[3 2 2 1 5 3]\n",
      "6\n",
      "------\n",
      "4\n",
      "len of x_train 43803\n",
      "-------\n",
      "Percentage of kmer sequences crossover: 99.99086820537406\n",
      "--- 0.002225637435913086 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from numpy import unique\n",
    "start_time = time.time()\n",
    "\n",
    "temp1 = [3, 2, 2, 1, 5, 3]\n",
    "temp1 = np.asarray(temp1)\n",
    "\n",
    "def unique_by_first(a):\n",
    "    tmp = a.reshape(a.shape[0], -1)\n",
    "    b = np.ascontiguousarray(tmp).view(np.dtype((np.void, tmp.dtype.itemsize * tmp.shape[1])))\n",
    "    _, idx, inverse = np.unique(b, return_index=True, return_inverse=True)\n",
    "    return  a[idx].reshape(-1, *a.shape[1:]), inverse\n",
    "\n",
    "uniqueElements_x_train, indexes_x_train = unique_by_first(temp1)\n",
    "\n",
    "\n",
    "print(uniqueElements_x_train)\n",
    "print(indexes_x_train)\n",
    "print(uniqueElements_x_train[indexes_x_train])\n",
    "print(len(indexes_x_train))\n",
    "print(\"------\")\n",
    "print(len(uniqueElements_x_train))\n",
    "print(\"len of x_train\", len(x_train))\n",
    "\n",
    "print(\"-------\")\n",
    "print(\"Percentage of kmer sequences crossover:\", ((len(x_train) - len(uniqueElements_x_train))/len(x_train)) *100)\n",
    "\n",
    "\n",
    "def removeDuplicatesWithY(indexesList, x_data, y_data):\n",
    "    setOfIndexes = {}\n",
    "    \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x7f2e10235510>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'filter' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-73bd9701a433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'filter' has no len()"
     ]
    }
   ],
   "source": [
    "from numpy import argsort\n",
    "from numpy import unique\n",
    "from numpy import split\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "idx_sort = argsort(x_train)\n",
    "sorted_records_array = x_train[idx_sort]\n",
    "vals, idx_start, count = unique(sorted_records_array, return_counts=True,\n",
    "                                return_index=True)\n",
    "\n",
    "# sets of indices\n",
    "res = split(idx_sort, idx_start[1:])\n",
    "#filter them with respect to their size, keeping only items occurring more than once\n",
    "\n",
    "vals = vals[count > 1]\n",
    "res = filter(lambda x: x.size > 1, res)\n",
    "\n",
    "print(res)\n",
    "print(len(res))\n",
    "print(vals)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'add' requires a 'set' object but received a 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-292533507abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msizeHidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'add' requires a 'set' object but received a 'tuple'"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "\n",
    "sizeHidden = len(x_hidden)\n",
    "sizeTest = len(x_test)\n",
    "\n",
    "#precondition: x_train has to be bigger than hidden and test which should always be true\n",
    "for i in range(len(x_train)):\n",
    "    \n",
    "    set.add(tuple(x_train[i]))\n",
    "    \n",
    "    if i < sizeHidden:\n",
    "        set.add(x_hidden[i])\n",
    "    \n",
    "    if i < sizeTest:\n",
    "        set.add(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell defines the CNN model\n",
    "\n",
    "#import statements\n",
    "from keras.layers import Dense, Conv1D, Flatten, Input, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#loss function\n",
    "def masked_loss_function(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "    return K.binary_crossentropy(y_true * mask, y_pred * mask)\n",
    "\n",
    "\n",
    "#defining the model\n",
    "def CNNModel(input_shape):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = (X_input)\n",
    "    \n",
    "    \n",
    "    #Layer1\n",
    "    #change to conv2d\n",
    "    X = Conv2D(16, (20, 4), name = 'conv1', strides = (1, 1), activation = 'relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    \n",
    "    #Layer2\n",
    "    X = Conv2D(16, (16, 4), name = 'conv2', strides = (1, 1), activation = 'relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "  \n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(3, activation='relu', name='fc')(X) #final predictions\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='CNNModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNNModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 1000, 40, 4)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 981, 37, 16)       5136      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 981, 37, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 966, 34, 32)       32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 966, 34, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 959, 31, 64)       65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 959, 31, 64)       256       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1902656)           0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 3)                 5707971   \n",
      "=================================================================\n",
      "Total params: 5,811,955\n",
      "Trainable params: 5,811,731\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "1000, 40, 4\n",
    "mod = CNNModel([numOfSequences, kMerLength, oneHotEncoding])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    Classifier interface.\n",
    "    Args:\n",
    "    Attributes:\n",
    "        get_inputs (list): a list of input names.\n",
    "            Derived from model_inputs unless implemented.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def get_inputs(self):\n",
    "        return model_inputs[self.__class__.__name__]\n",
    "    def __init__(self, **hyperparameters):\n",
    "        pass\n",
    "    def save(self, prefix):\n",
    "        arch_fname = prefix + '.arch.json'\n",
    "        weights_fname = prefix + '.weights.h5'\n",
    "        open(arch_fname, 'w').write(self.model.to_json())\n",
    "        self.model.save_weights(weights_fname, overwrite=True)\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "    def get_placeholder_inputs(self):\n",
    "        \"\"\"Returns dictionary of named keras inputs\"\"\"\n",
    "        return collections.OrderedDict(\n",
    "            [(name, placeholder_dict[name])\n",
    "             for name in self.get_inputs])\n",
    "class SequenceClassifier(Classifier):\n",
    "    def __init__(self,num_tasks=1,\n",
    "                 num_filters=(15, 15, 15), conv_width=(15, 15, 15),\n",
    "                 pool_width=35, dropout=0, batch_norm=False):\n",
    "        assert len(num_filters) == len(conv_width)\n",
    "        # # configure inputs\n",
    "        ##Dictionary of data type to placeholder\n",
    "        self.inputs = self.get_placeholder_inputs()\n",
    "        # convolve sequence\n",
    "        seq_preds = self.inputs[\"data/genome_data_dir\"]\n",
    "        for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):\n",
    "            seq_preds = Conv1D(\n",
    "                nb_filter, nb_col, kernel_initializer='he_normal')(seq_preds)\n",
    "            if batch_norm:\n",
    "                seq_preds = BatchNormalization()(seq_preds)\n",
    "            seq_preds = Activation('relu')(seq_preds)\n",
    "            if dropout > 0:\n",
    "                seq_preds = Dropout(dropout)(seq_preds)\n",
    "        # pool and fully connect\n",
    "        seq_preds = AveragePooling1D((pool_width))(seq_preds)\n",
    "        seq_preds = Flatten()(seq_preds)\n",
    "        seq_preds = Dense(output_dim=num_tasks)(seq_preds)\n",
    "        self.logits=seq_preds\n",
    "        preds = Activation('sigmoid')(self.logits)\n",
    "        self.model = Model(inputs=self.inputs.values(), outputs=preds)\n",
    "    def get_logits(self):\n",
    "    \treturn self.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basepairmodels]",
   "language": "python",
   "name": "conda-env-basepairmodels-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
